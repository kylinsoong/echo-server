= User Guide
:toc: manual

== Installation

Install on Red Hat Linux via:

----
rpm -ivh ttcp-1.12-1.x86_64.rpm 
----

== Run

=== ttcp

[source, bash]
.*Run as Receiver*
----
ttcp -r -s -4 -v -p 5050
----

[source, bash]
.*Run as Transfer*
----
ttcp -t -s -n 10240 -v -4 -p 5050 10.1.20.203
----

[source, bash]
.*Collect Results*
----
ttcp-r: 83886080 bytes in 1.34 real seconds = 61108.02 KB/sec +++
ttcp-r: 83886080 bytes in 0.10 CPU seconds = 827040.34 KB/cpu sec
ttcp-r: 10341 I/O calls, msec/call = 0.13, calls/sec = 7713.84
ttcp-r: 0.0user 0.0sys 0:01real 6% 0i+0d 330maxrss 0+2pf 368+0csw

ttcp-t: 83886080 bytes in 1.29 real seconds = 63287.18 KB/sec +++
ttcp-t: 83886080 bytes in 0.03 CPU seconds = 2522633.49 KB/cpu sec
ttcp-t: 10240 I/O calls, msec/call = 0.13, calls/sec = 7910.90
ttcp-t: 0.0user 0.0sys 0:01real 1% 0i+0d 362maxrss 0+2pf 122+0csw
----

=== echoS

[source, bash]
.*Run as Server*
----
# echoS -s -p 12302 -4
----

[source, bash]
.*Run as Client*
----
# echoS -c -p 12302 -P 23401 10.1.20.203
1649260296 1962 echoS: socket
1649260296 1962 echoS: bind
1649260296 1962 echoS: connect
time
1649260300
daytime
Wed Apr  6 23:51:44 2022
chargen
u$a71i0Rkk*1LkQ46d2Dqtau4Pn1cU;tZ8G'#Xsn_};-&)\<
----

=== chat

[source, bash]
.*Run as Server*
----
# chat -s -p 12303 -4
chat-s: socket
chat-s: bind
chat-s: accept from 10.1.20.201
10.1.20.201: hi
chat-s: yes
----

[source, bash]
.*Run as Client*
----
# chat -c -p 12303 -P 23401 10.1.20.203
chat-c: socket
chat-c: bind
chat-c: connect
chat-c: hi
10.1.20.203: yes
----

== Prerequisites

=== Lab Topologies

As showing in figure, there are 4 hosts, `lb-1`, `lb-2`, `app-1`, `app-2`, as per host name suggestions, 2 hosts for proxy/loadbalancer, 2 hosts for apps. The hosts for proxy/loadbalancer has 2 NICs, ens33 for public access, and ens34 for NAT or internal route usage. `lb-1` and `lb-2` are in 10.1.10.0/24 network, all hosts are in 10.1.20.0/24 network.

image:img/tcp-lab-topology.png[]

=== Linux Kernel Networking Stack 

There are seven logical networking layers according to the Open Systems Interconnection (OSI) model. The lowest layer is the physical layer, which is the hardware, and the highest layer is the application layer, where userspace software processes are running. These seven layers are:

1. `The physical layer:` Handles electrical signals and the low level details.
2. `The data link layer:` Handles data transfer between endpoints. The most common data link layer is Ethernet. The Linux Ethernet *network device drivers* reside in this layer.
3. `The network layer:` Handles packet forwarding and host addressing. The most common network layers of the Linux Kernel Networking subsystem: *IPv4* or *IPv6*.
4. `The transport layer:` Handles data sending between nodes. The *TCP* and *UDP* protocols are the best-known protocols.
5. `The session layer:` Handles sessions between endpoints.
6. `The presentation layer:` Handles delivery and formatting.
7. `The application layer:` Provides network services to end-user applications.

Linux Kernel Networking stack handles 3 layers, the L2, L3 and L4, correspond to the data link layer, the network layer, and the transport layer in the seven-layer model. The essence of the Linux kernel stack is passing incoming packets from L2 (the network device drivers) to L3 (the network layer, usually IPv4 or IPv6) and then to L4 (the transport layer, where you have, for example, TCP or UDP listening sockets) if they are for local delivery, or back to L2 for transmission when the packets should be forwarded. Outgoing packets that were locally generated are passed from L4 to L3 and then to L2 for actual transmission by the network device driver. 

The kernel does not handle any layer above L4; those layers (the session, presentation, and application layers) are handled solely by userspace applications. The physical layer (L1) is also not handled by the Linux kernel.

image:img/linux-network.png[]

== Three-Way Handshaking

If a server_(eg, user space app on host `app-2`)_ request a *passive open*_(call sockets api tell TCP that it's ready to accept connection)_,

[source, bash]
----
ttcp -r 
----

a client _(on host `app-1` request)_ a *active open*_(call sockets api tell TCP that it needs to be connected to that particular server)_

[source, bash]
----
ttcp -t app-2
----

then TCP start Three-Way Handshaking:

1. client send SYN to server
2. server received client's SYN, send SYN + ACK to client
3. client received server's SYN + ACK , send ACK to server

image:img/tcp-3-times-handshake.png[]

=== How many times SYN Retransmission?

If set a policy on host `app-2` to DROP any packet if destination port is `5001`,

[source, bash]
----
iptables -I INPUT -p tcp --dport 5001 -j DROP
----

then client try to connection to server, Three-Way Handshaking will blocked on SYN sending, and end with `110 Connection timed out` error after 6 times Retransmission.

image:img/tcp-syn-retries.png[]

If look into above diagram, there are 7 SYN packets, 6 SYN are Retransmission packet, if look into the timestamp of each packet:

|===
|SYN Times |Timestamp |Time Interval comapre last SYN Times(seconds) 

|1
|12:13:01.849323
|

|2
|12:13:02.853415
|1

|3
|12:13:04.860668
|2

|4
|12:13:08.869242
|4

|5
|12:13:16.900881
|8

|6
|12:13:32.932824
|16

|7
|12:14:04.996790
|32

|===

Note that the time interval between 2 retransmissions are in *Exponential backoff*, this due the the spec definition.

There are 6 times retries that because Linux default tcp_syn_retries is 6,

[source, bash]
.*net.ipv4.tcp_syn_retries*
----
# sysctl net.ipv4.tcp_syn_retries
net.ipv4.tcp_syn_retries = 6
----

=== How many times SYN + ACK Retransmission?

If set a policy on host `app-1` to DROP any packet if destination port is `20001`,

[source, bash]
----
iptables -I INPUT -p tcp --dport 20001 -j DROP
----

while trans connect to recv append a `-P 20001`, to make SYN + ACK's destination port is 20001, and hint policy set above.

[source, bash]
----
ttcp -t -P 20001 app-2
----  

The connection also end with `110 Connection timed out` error after 6 times Retransmission, but the differenc is the server also send the SYN + ACK Retransmission.

image:img/tcp-syn-ack-retries.png[]

To further investigate what caused the `110 Connection timed out` error, use the `tcp.flags.syn == 1 and tcp.flags.ack == 0` to filter only *SYN* packet:

|===
|No |SYN Times |Timestamp |Time Interval comapre last SYN Times(seconds)

|1
|1
|23:49:02.592
|

|4
|2
|23:49:03.595
|1

|6
|3
|23:49:05.599
|2

|9
|4
|23:49:09.603
|4

|12
|5
|23:49:17.619
|8

|15
|6
|23:49:33.619
|16

|18
|7
|23:50:05.731
|32

|===   

It's obvious that the `net.ipv4.tcp_syn_retries = 6` seeting on host `app-1` matters, `110 Connection timed out` error generated after 6 times of SYN Retransmission, *Exponential backoff* formular also be applied, the Time Interval comapre last SYN Times are 1, 2, 4, 8, 16, 32.

image:img/tcp-retransmission-expon.png[]  

The linux network stack also has a `net.ipv4.tcp_synack_retries` kernel parameter, the default value is 5, which means there should be 5 times SYN + ACK Retransmission in 31 seconds.

[source, bash]
----
# sysctl net.ipv4.tcp_synack_retries
net.ipv4.tcp_synack_retries = 5
----

Cause the SYN Retransmission will trigger SYN + ACK Retransmission time reset, so to investigate how many times SYN + ACK Retransmission, we need enlarge the SYN retries in trans side via

[source, bash]
----
sysctl -w net.ipv4.tcp_syn_retries=9
----

this will make the time interval between 6th retry and 7th retry are 32 seconds, the time interval between 7th retry and 8th retry are 62 seconds, which has enough time to view SYN + ACK retransmission, the below diagram show 5 times retransmission:

image:img/tcp-synack-retries.png[]

Linux also has a `net.ipv4.tcp_synack_retries` to control synack retries, if look into the time interval between 2 retransmissions, it also comply with with the *Exponential backoff* rule.

[source, bash]
.*net.ipv4.tcp_synack_retries*
----
# sysctl net.ipv4.tcp_synack_retries
net.ipv4.tcp_synack_retries = 5
----

=== Reset in Three-Way Handshaking

With the Linux TCP stack, if the timeout or retries occurred, the user space api will get a error(probably the usr space app need to handle error well). But if the client and server communication via a proxy, or firewall, the timeout or retries will end with a reset packet in most of occasion.


If set a policy on host `app-2` to REJECT any packet if destination port is `5001`,

[source, bash]
----
iptables -I INPUT -p tcp --dport 5001 -j REJECT --reject-with tcp-reset
----

than client will receive the RST packet from server.

image:img/tcp-syn-reset.png[]

If set a policy on host `app-1` to REJECT any packet if destination port is `20001`,

[source, bash]
----
iptables -I INPUT -p tcp --dport 20001 -j REJECT --reject-with tcp-reset
----

then start the client on host `app-1` with setting a client port, connect to server on host `app-2`

[source, bash]
----
ttcp -t -P 20001 app-2
----

the will trigger TCP Retransmission 6 times due to linux network stack ipv4 setting.

image:img/tcp-synack-reset.png[]

* Why reset not ending the tcp connection? 
** Because the Client TCP Stack not receive the `RST`, the client TCP stack wait the `SYN + ACK` from server and never receives, the `SYN + ACK` be reject by firewall and send `RST` to server.

* Why client retries 6 times?
** Because the client host `app-1` has setting retries times via `net.ipv4.tcp_syn_retries=6`.

* Why server didn't retransmission?
** Because the TCP Satck on server side received the `RST` packet.

* Why *TCP Port numbers reused*?
** Server port reused due to
** client port reused duo to retransmission.

* Why *TCP Previous segment not captured*?
** The Client not received `SYN + ACK`.  



 

== Data Transfer 

=== sequence number and acknowledgment number

[cols="5a,5a"]
|===
|sequence number |acknowledgment number

|

* The number of the first data byte contained in that segment
* A random number bewteen 0 and 2^32
* Control segment(connection establishment, termination, or abortion) also has a sequence number, but no data packet
* sequence number peered with acknowledgment number for Flow Control and Error Control

|

* The number of next byte that the receiver want to recive

|===

[source, bash]
.*Start the ttcp receiver*
----
ttcp -r -4 -v -p 12301 
----

[source, bash]
.*Start the ttcp transfer, sending 3 2 bytes sgement,*
----
# ttcp -t -v -4 -p 12301 -P 23401 10.1.20.203
ttcp-t: buflen=8192, nbuf=2048, align=16384/0, port=12301  tcp  -> 10.1.20.203
ttcp-t: socket
ttcp-t: connect
a
b
c
----

image:img/seq-ack.jpg[]

* Control segment: seq number is `2715613001`, and the ack number from receiver is `2715613002`
* 1st data segment: seq number is `1`, and the ack number from server is `3`
* 2nd data segment: seq number is `3`, and the ack number from receiver is `5`
* 3rd data segment: seq number is `5`, and the ack number from receiver is `7`

Use the ttcp to send 5 continue segments, each with size of 1000 bytes.

[source, bash]
.*Start the ttcp receiver*
----
ttcp -r -4 -v -p 12301 -l 1000 -n 5 -s
----

[source, bash]
.*Start the ttcp transfer*
----
ttcp -t -v -4 -p 12301 -P 23401 -l 1000 -n 5 -w 1000 -s 10.1.20.203
----

image:img/seq-ack-2seg.png[]

|===
|no |seq number |ack number

|1
|1
|1001

|2
|1001
|2001

|3
|2001
|3001

|4
|3001
|4001

|5
|4001
|5001
|===

=== How TCP recv buffer size affect packet processing

Send 10 MB size large message 50 times(total 500 MB in size), and record the time if taffic processed per seconds(TPS), run 3 times for each specific recv buffer size, and caculate the avarage TPS.

[source, bash]
----
ttcp -r -4 -l 10485760 -n 50 -p 10000 -s -b 21845 -v
ttcp -t -l 10485760 -n 50 -p 10000 -s  10.1.20.204
----

*  `-b` with recv side will set the recv buffer size
*  `-l 10485760` - single message size, 10 MB
*  `-n 50` - how many messages be sent, 50

|===
|SO_RCVBUF |KB/sec|KB/sec|KB/sec |AVG TPS(MB/sec)

|87380
|50773.00
|49461.45
|49192.62
|50

|43690
|47195.15
|46548.71
|46734.77
|47

|21845
|36026.04
|35583.06
|36256.20
|36

|10923
|24055.33
|23748.42
|23106.25
|24

|5460
|5144.34
|5159.72
|5148.27
|5.2

|2730
|3836.44
|-
|-
|3.8

|1365
|2390.78
|-
|-
|2.4

|===

=== How TCP send buffer size affect packet processing

Send 1 MB size large message 50 times(total 50 M in size), and record the time if taffic processed per seconds(TPS), run 3 times for each specific send buffer size, and caculate the avarage TPS.

[source, bash]
----
ttcp -r -4 -l 1048576 -n 50 -p 10000 -s -v
ttcp -t -l 1048576 -n 50 -p 10000 -s -b 16384 10.1.20.204
----

*  `-b` with recv side will set the recv buffer size
*  `-l 1048576` - single message size, 1 MB
*  `-n 50` - how many messages be sent, 50

|===
|SO_SNDBUF |KB/sec|KB/sec|KB/sec |AVG TPS(MB/sec)

|32768
|56403.51
|57073.74
|54878.68
|56

|16384
|50773.00
|49461.45
|49192.62
|50

|12288
|870.79
|757.07
|-
|0.8

|8192
|870.54
|606.97
|871.09
|0.8

|===

Conclusion fo snd/rcv buf to affect the tcp traffic:

* The default snd/rcv buf size(16384/87380) have well performance
* Decrease send buffer size affect tcp traffic significantly.

=== How MSS affect packet processing

MSS(maximum segment size) is a parameter of the options field of the TCP header that specifies the largest amount of data. 

----
MSS = MTU - 20 -20
----

MTU is the size of the largest protocol data unit (PDU) that can be communicated in a single network layer transaction. TCP has regular 20 bytes headers, and IP always has 20 bytes headers,

[source, bash] 
.*View default MTU*
----
# ifconfig ens33| grep mtu
ens33: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500
----

[source, bash]
.*Set MTP a value*
----
ifconfig ens33 mtu 1300
----

Traffic process per seconds:

|===
|MTU |MSS |KB/sec 

|1500
|1460
|51520.98

|1300
|1260
|3665.92 

|1100
|1060
|976.42

|900
|860
|712.74
|===

* Linux TCP stack optimize the traffic processing with default MTU 1500.

=== What's the Relationship between IP Datagram and TCP Segment Sizes?

Base on rfc879, the relationship between the value of the maximum IP datagram size and the maximum TCP segment size is obscure.  The problem is that both the IP header and the TCP header may vary in length. The TCP Maximum Segment Size option (MSS) is defined to specify the maximum number of data octets in a TCP segment exclusive of TCP (or IP) header.

To notify the data sender of the largest TCP segment it is possible to receive the calculation of the MSS value to send is:

[source, bash]
----
MSS = MTU - sizeof(TCPHDR) - sizeof(IPHDR)
----

On receipt of the MSS option the calculation of the size of segment that can be sent is:

[source, bash]
----
SndMaxSegSiz = MIN((MTU - sizeof(TCPHDR) - sizeof(IPHDR)), MSS)
----

If MTU use default 1500, size of TCP Header is 32, IP header is 20, so the MSS is 1460, the max send segment is 1448, the following is 1447 bytes data, append enter is equal the max send segment.

[source, text]
.*1447 lenth data*
----
0123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456780123456
----

While sending above data over ttcp, it will get the max send segment.

image:img/tcp-max-send-segment.png[]

=== How timestamps affect the mss size?

There can be up to 40 bytes optional information in TCP Header, the TCP Timestamps options usually inserted as 3rd option in 8 bytes long, if TCP Timestamps is enabled, the 1st and 2nd option will set a NoP(No-Operation) options, each NoP option will take 2 bytes, so enable TCP Timestamps will take additional 12 bytes(2 + 2 + 8), this will make TCP headers as 32 bytes.

Run ttcp recv on host `app-2`, run ttcp trans on host `app-1`, specify to send only 1 message:

[source, bash]
----
ttcp -r
----

[source, bash]
----
ttcp -t 10.1.20.204
----

Note that, the above data transfer has TCP Timestamps enabled, use `tcpdump` capture a data transfer pcap.

Execute the below command on hosts that run ttcp:

[source, bash]
----
~]# sysctl -w net.ipv4.tcp_timestamps=0
net.ipv4.tcp_timestamps = 0
----

Then run ttcp recv on host `app-2`, ttcp trans on host `app-1`, specify to send only 1 message, this time will make data transfer has TCP Timestamps disabled, use `tcpdump` capture a data transfer pcap.

image:img/tcp_timestamps.png[]

Turn timestamps off to reduce performance spikes related to timestamp generation. The sysctl command controls the values of TCP related entries, setting the timestamps kernel parameter found at `/proc/sys/net/ipv4/tcp_timestamps`.

Turn timestamps off with the following command:

[source, bash]
----
~]# sysctl -w net.ipv4.tcp_timestamps=0
net.ipv4.tcp_timestamps = 0
----

Turn timestamps on with the following command:

[source, bash]
----
~]# sysctl -w net.ipv4.tcp_timestamps=1
net.ipv4.tcp_timestamps = 1
----

Print the current value with the following command:

[source, bash]
----
~]# sysctl net.ipv4.tcp_timestamps
net.ipv4.tcp_timestamps = 1
----

The value 1 indicates that timestamps are on, the value 0 indicates they are off.






== F5 BIG-IP fastl4 profie

=== Reset on timeout

If set the reset-on-timeout to enable, and specify a idle timeout,

[source, bash]
----
reset-on-timeout enabled
idle-timeout 300
----

the system sends a reset packet (RST) when a connection exceeds the idle timeout value.

image:img/reset-idle-timeout.png[]

=== MSS Overwrite

If set mss-override enable and set a value,

[source, bash]
----
mss-override 256
----

than the Proxy will overwrite MSS(Maximum segment size), the smaller mss, the lower traffic processing capibility. The following are comparision between default MSS, and 256 bytes mss(send 3 MB data):

|===
|options |default(1460) |overwrite(256)

|Total Packets
|274
|849

|TPS (KB/sec)
|15678.19
|4855.24

|Time (seconds)
|0.20
|0.63

|CPU Time (seconds)
|0.04
|0.09
|===

The MSS is specified as a TCP option initially as TCP SYN packet.

image:img/mss-overwrite.png[]

