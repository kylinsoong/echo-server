= User Guide
:toc: manual

== Installation

Install on Red Hat Linux via:

----
rpm -ivh l4-client-server-1.1-1.x86_64.rpm
----

== Run

=== ttcp

[source, bash]
.*Run as Receiver*
----
ttcp -r -s -4 -v -p 5050
----

[source, bash]
.*Run as Transfer*
----
ttcp -t -s -n 10240 -v -4 -p 5050 10.1.20.203
----

[source, bash]
.*Collect Results*
----
ttcp-r: 83886080 bytes in 1.34 real seconds = 61108.02 KB/sec +++
ttcp-r: 83886080 bytes in 0.10 CPU seconds = 827040.34 KB/cpu sec
ttcp-r: 10341 I/O calls, msec/call = 0.13, calls/sec = 7713.84
ttcp-r: 0.0user 0.0sys 0:01real 6% 0i+0d 330maxrss 0+2pf 368+0csw

ttcp-t: 83886080 bytes in 1.29 real seconds = 63287.18 KB/sec +++
ttcp-t: 83886080 bytes in 0.03 CPU seconds = 2522633.49 KB/cpu sec
ttcp-t: 10240 I/O calls, msec/call = 0.13, calls/sec = 7910.90
ttcp-t: 0.0user 0.0sys 0:01real 1% 0i+0d 362maxrss 0+2pf 122+0csw
----

=== echoS

[source, bash]
.*Run as Server*
----
# echoS -s -p 12302 -4
----

[source, bash]
.*Run as Client*
----
# echoS -c -p 12302 -P 23401 10.1.20.203
1649260296 1962 echoS: socket
1649260296 1962 echoS: bind
1649260296 1962 echoS: connect
time
1649260300
daytime
Wed Apr  6 23:51:44 2022
chargen
u$a71i0Rkk*1LkQ46d2Dqtau4Pn1cU;tZ8G'#Xsn_};-&)\<
----

=== chat

[source, bash]
.*Run as Server*
----
# chat -s -p 12303 -4
chat-s: socket
chat-s: bind
chat-s: accept from 10.1.20.201
10.1.20.201: hi
chat-s: yes
----

[source, bash]
.*Run as Client*
----
# chat -c -p 12303 -P 23401 10.1.20.203
chat-c: socket
chat-c: bind
chat-c: connect
chat-c: hi
10.1.20.203: yes
----

== Prerequisites

=== Linux Networking

image:img/linux-network.png[]

== Three-Way Handshaking

If a server_(eg, user space app on host `app-2`)_ request a *passive open*_(call sockets api tell TCP that it's ready to accept connection)_,

[source, bash]
----
ttcp -r 
----

a client _(on host `app-1` request)_ a *active open*_(call sockets api tell TCP that it needs to be connected to that particular server)_

[source, bash]
----
ttcp -t app-2
----

then TCP start Three-Way Handshaking:

1. client send SYN to server
2. server received client's SYN, send SYN + ACK to client
3. client received server's SYN + ACK , send ACK to server

image:img/tcp-3-times-handshake.png[]

=== How many times SYN Retransmission?

If set a policy on host `app-2` to DROP any packet if destination port is `5001`,

[source, bash]
----
iptables -I INPUT -p tcp --dport 5001 -j DROP
----

then client try to connection to server, Three-Way Handshaking will blocked on SYN sending, and end with `110 Connection timed out` error after 6 times Retransmission.

image:img/tcp-syn-retries.png[]

If look into above diagram, there are 7 SYN packets, 6 SYN are Retransmission packet, if look into the timestamp of each packet:

|===
|SYN Times |Timestamp |Time Interval comapre last SYN Times(seconds) 

|1
|12:13:01.849323
|

|2
|12:13:02.853415
|1

|3
|12:13:04.860668
|2

|4
|12:13:08.869242
|4

|5
|12:13:16.900881
|8

|6
|12:13:32.932824
|16

|7
|12:14:04.996790
|32

|===

Note that the time interval between 2 retransmissions are in *Exponential backoff*, this due the the spec definition.

There are 6 times retries that because Linux default tcp_syn_retries is 6,

[source, bash]
.*net.ipv4.tcp_syn_retries*
----
# sysctl net.ipv4.tcp_syn_retries
net.ipv4.tcp_syn_retries = 6
----

=== How many times SYN + ACK Retransmission?

If set a policy on host `app-1` to DROP any packet if destination port is `20001`,

[source, bash]
----
iptables -I INPUT -p tcp --dport 20001 -j DROP
----

while client connect to server append a `-P 20001`, to make SYN + ACK's destination port is 20001, and hint policy set above.

[source, bash]
----
ttcp -t -P 20001 app-2
----  

The connection also end with `110 Connection timed out` error after 6 times Retransmission, but the differenc is the server also send the SYN + ACK Retransmission.

image:img/tcp-syn-ack-retries.png[]

To investigate how many times SYN + ACK Retransmission, we need enlarge the SYN retries in client side via

[source, bash]
----
sysctl -w net.ipv4.tcp_syn_retries=8
----

this will make sure the time interval between 7th retry and 8th retry are around 120 seconds, which has enough time to view SYN + ACK retransmission, the below diagram show 5 times retransmission.

image:img/tcp-synack-retries.png[]

Linux also has a `net.ipv4.tcp_synack_retries` to control synack retries, if look into the time interval between 2 retransmissions, it also comply with with the *Exponential backoff* rule.

[source, bash]
.*net.ipv4.tcp_synack_retries*
----
# sysctl net.ipv4.tcp_synack_retries
net.ipv4.tcp_synack_retries = 5
----

=== Reset in Three-Way Handshaking

With the Linux TCP stack, if the timeout or retries occurred, the user space api will get a error(probably the usr space app need to handle error well). But if the client and server communication via a proxy, or firewall, the timeout or retries will end with a reset packet in most of occasion.


If set a policy on host `app-2` to REJECT any packet if destination port is `5001`,

[source, bash]
----
iptables -I INPUT -p tcp --dport 5001 -j REJECT --reject-with tcp-reset
----

than client will receive the RST packet from server.

image:img/tcp-syn-reset.png[]

If set a policy on host `app-1` to REJECT any packet if destination port is `20001`,

[source, bash]
----
iptables -I INPUT -p tcp --dport 20001 -j REJECT --reject-with tcp-reset
----

then start the client on host `app-1` with setting a client port, connect to server on host `app-2`

[source, bash]
----
ttcp -t -P 20001 app-2
----

the will trigger TCP Retransmission 6 times due to linux network stack ipv4 setting.

image:img/tcp-synack-reset.png[]

* Why reset not ending the tcp connection? 
** Because the Client TCP Stack not receive the `RST`, the client TCP stack wait the `SYN + ACK` from server and never receives, the `SYN + ACK` be reject by firewall and send `RST` to server.

* Why client retries 6 times?
** Because the client host `app-1` has setting retries times via `net.ipv4.tcp_syn_retries=6`.

* Why server didn't retransmission?
** Because the TCP Satck on server side received the `RST` packet.

* Why *TCP Port numbers reused*?
** Server port reused due to
** client port reused duo to retransmission.

* Why *TCP Previous segment not captured*?
* The Client not received `SYN + ACK`.  



 

== TCP 

=== sequence number and acknowledgment number

[cols="5a,5a"]
|===
|sequence number |acknowledgment number

|

* The number of the first data byte contained in that segment
* A random number bewteen 0 and 2^32
* Control segment(connection establishment, termination, or abortion) also has a sequence number, but no data packet
* sequence number peered with acknowledgment number for Flow Control and Error Control

|

* The number of next byte that the receiver want to recive

|===

[source, bash]
.*Start the ttcp receiver*
----
ttcp -r -4 -v -p 12301 
----

[source, bash]
.*Start the ttcp transfer, sending 3 2 bytes sgement,*
----
# ttcp -t -v -4 -p 12301 -P 23401 10.1.20.203
ttcp-t: buflen=8192, nbuf=2048, align=16384/0, port=12301  tcp  -> 10.1.20.203
ttcp-t: socket
ttcp-t: connect
a
b
c
----

image:img/seq-ack.jpg[]

* Control segment: seq number is `2715613001`, and the ack number from receiver is `2715613002`
* 1st data segment: seq number is `1`, and the ack number from server is `3`
* 2nd data segment: seq number is `3`, and the ack number from receiver is `5`
* 3rd data segment: seq number is `5`, and the ack number from receiver is `7`

Use the ttcp to send 5 continue segments, each with size of 1000 bytes.

[source, bash]
.*Start the ttcp receiver*
----
ttcp -r -4 -v -p 12301 -l 1000 -n 5 -s
----

[source, bash]
.*Start the ttcp transfer*
----
ttcp -t -v -4 -p 12301 -P 23401 -l 1000 -n 5 -w 1000 -s 10.1.20.203
----

image:img/seq-ack-2seg.png[]

|===
|no |seq number |ack number

|1
|1
|1001

|2
|1001
|2001

|3
|2001
|3001

|4
|3001
|4001

|5
|4001
|5001
|===
 

=== Four Times Handshake

The client send 'exit' signal to server will simulate a *Active Close*, accordingly the server send 'exit' signal to client will simulate a *Passive Close*.

[source, bash]
.*Active Close*
----
17:31:31.908950 IP chat-client.42620 > chat-server.8878: Flags [F.], seq 81, ack 1, win 229, options [nop,nop,TS val 13331 ecr 584320], length 0
17:31:31.949050 IP chat-server.8878 > chat-client.42620: Flags [.], ack 82, win 227, options [nop,nop,TS val 586362 ecr 13331], length 0
17:31:33.909179 IP chat-server.8878 > chat-client.42620: Flags [F.], seq 1, ack 82, win 227, options [nop,nop,TS val 588322 ecr 13331], length 0
17:31:33.910129 IP chat-client.42620 > chat-server.8878: Flags [.], ack 2, win 229, options [nop,nop,TS val 15332 ecr 588322], length 0
----

[source, bash]
.*Passive Close*
----
17:33:21.541507 IP chat-server.8878 > chat-client.42622: Flags [F.], seq 81, ack 81, win 227, options [nop,nop,TS val 695954 ecr 120954], length 0
17:33:21.582513 IP chat-client.42622 > chat-server.8878: Flags [.], ack 82, win 229, options [nop,nop,TS val 122995 ecr 695954], length 0
17:33:23.542677 IP chat-client.42622 > chat-server.8878: Flags [F.], seq 81, ack 82, win 229, options [nop,nop,TS val 124954 ecr 695954], length 0
17:33:23.542703 IP chat-server.8878 > chat-client.42622: Flags [.], ack 82, win 227, options [nop,nop,TS val 697955 ecr 124954], length 0
----

=== How TCP recv buffer size affect packet processing

Send 10 MB size large message 50 times(total 500 MB in size), and record the time if taffic processed per seconds(TPS), run 3 times for each specific recv buffer size, and caculate the avarage TPS.

[source, bash]
----
ttcp -r -4 -l 10485760 -n 50 -p 10000 -s -b 21845 -v
ttcp -t -l 10485760 -n 50 -p 10000 -s  10.1.20.204
----

*  `-b` with recv side will set the recv buffer size
*  `-l 10485760` - single message size, 10 MB
*  `-n 50` - how many messages be sent, 50

|===
|SO_RCVBUF |KB/sec|KB/sec|KB/sec |AVG TPS(MB/sec)

|87380
|50773.00
|49461.45
|49192.62
|50

|43690
|47195.15
|46548.71
|46734.77
|47

|21845
|36026.04
|35583.06
|36256.20
|36

|10923
|24055.33
|23748.42
|23106.25
|24

|5460
|5144.34
|5159.72
|5148.27
|5.2

|2730
|3836.44
|-
|-
|3.8

|1365
|2390.78
|-
|-
|2.4

|===

=== How TCP send buffer size affect packet processing

Send 1 MB size large message 50 times(total 50 M in size), and record the time if taffic processed per seconds(TPS), run 3 times for each specific send buffer size, and caculate the avarage TPS.

[source, bash]
----
ttcp -r -4 -l 1048576 -n 50 -p 10000 -s -v
ttcp -t -l 1048576 -n 50 -p 10000 -s -b 16384 10.1.20.204
----

*  `-b` with recv side will set the recv buffer size
*  `-l 1048576` - single message size, 1 MB
*  `-n 50` - how many messages be sent, 50

|===
|SO_SNDBUF |KB/sec|KB/sec|KB/sec |AVG TPS(MB/sec)

|32768
|56403.51
|57073.74
|54878.68
|56

|16384
|50773.00
|49461.45
|49192.62
|50

|12288
|870.79
|757.07
|-
|0.8

|8192
|870.54
|606.97
|871.09
|0.8

|===

Conclusion fo snd/rcv buf to affect the tcp traffic:

* The default snd/rcv buf size(16384/87380) have well performance
* Decrease send buffer size affect tcp traffic significantly.

=== MSS

MSS(maximum segment size) is a parameter of the options field of the TCP header that specifies the largest amount of data. 

----
MSS = MTU - 20 -20
----

MTU is the size of the largest protocol data unit (PDU) that can be communicated in a single network layer transaction. TCP has regular 20 bytes headers, and IP always has 20 bytes headers,

[source, bash] 
.*View default MTU*
----
# ifconfig ens33| grep mtu
ens33: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500
----

[source, bash]
.*Set MTP a value*
----
ifconfig ens33 mtu 1300
----

Traffic process per seconds:

|===
|MTU |MSS |KB/sec 

|1500
|1460
|51520.98

|1300
|1260
|3665.92 

|1100
|1060
|976.42

|900
|860
|712.74
|===

* Linux TCP stack optimize the traffic processing with default MTU 1500.



== F5 BIG-IP fastl4 profie

=== Reset on timeout

If set the reset-on-timeout to enable, and specify a idle timeout,

[source, bash]
----
reset-on-timeout enabled
idle-timeout 300
----

the system sends a reset packet (RST) when a connection exceeds the idle timeout value.

image:img/reset-idle-timeout.png[]

=== MSS Overwrite

If set mss-override enable and set a value,

[source, bash]
----
mss-override 256
----

than the Proxy will overwrite MSS(Maximum segment size), the smaller mss, the lower traffic processing capibility. The following are comparision between default MSS, and 256 bytes mss(send 3 MB data):

|===
|options |default(1460) |overwrite(256)

|Total Packets
|274
|849

|TPS (KB/sec)
|15678.19
|4855.24

|Time (seconds)
|0.20
|0.63

|CPU Time (seconds)
|0.04
|0.09
|===

The MSS is specified as a TCP option initially as TCP SYN packet.

image:img/mss-overwrite.png[]

